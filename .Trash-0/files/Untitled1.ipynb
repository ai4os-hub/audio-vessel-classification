{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd322f1b-583a-4fe9-802a-cbe577c799e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audio_vessel_classifier.models import model_loader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b8bc9-6d89-46b8-b004-5b4aa7330447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_data(args):\n",
    "\n",
    "    if args[\"model_choice\"]==\"fine_tuning\":\n",
    "        freeze=False\n",
    "    else:\n",
    "        freeze= True\n",
    "\n",
    "            \n",
    "    if args[\"embedding_file\"]:\n",
    "        embedded=True\n",
    "    if args[\"audio_file\"]:\n",
    "\n",
    "        # Configuration\n",
    "        duration = 10  # in seconds\n",
    "        overlap = 0.2\n",
    "        desired_fs = 48000\n",
    "        channel = 0\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Paths\n",
    "        wav_path=\"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-04-18_23-11-14_414-573716_Cargo_underway-using-engine_11-0_4116.wav\"\n",
    "        # Load waveform\n",
    "        waveform_info = torchaudio.info(wav_path)\n",
    "        waveform, fs = torchaudio.load(wav_path)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if waveform_info.sample_rate != desired_fs:\n",
    "            transform = torchaudio.transforms.Resample(fs, desired_fs)\n",
    "            waveform = transform(waveform)\n",
    "        \n",
    "        # Crop to duration\n",
    "        max_samples = int(duration * desired_fs)\n",
    "        \n",
    "        waveform = waveform[channel, :max_samples]\n",
    "        if waveform.shape[0] < max_samples:\n",
    "            waveform = F.pad(waveform, (0, max_samples - waveform.shape[0]))\n",
    "        \n",
    "        # Convert to numpy for processor\n",
    "        x = waveform.cpu().numpy()\n",
    "        \n",
    "        # Initialize processor and model\n",
    "        processor = ClapProcessor.from_pretrained(\"davidrrobinson/BioLingual\")\n",
    "        clap = ClapAudioModelWithProjection.from_pretrained(\"davidrrobinson/BioLingual\")\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(audios=[x], return_tensors=\"pt\", sampling_rate=desired_fs, padding=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        if freeze:\n",
    "            # Extract embedding\n",
    "            with torch.no_grad():\n",
    "                embedding = clap(**inputs).audio_embeds\n",
    "        else:\n",
    "            embedding= inputs[\"input_features\"]\n",
    "        print(\"Embedding shape:\", embedding.shape)\n",
    "        embedded=True\n",
    "    if embedded==True:\n",
    "        logger.debug(\"Predict with args: %s\", args)\n",
    "        device = return_device()\n",
    "    \n",
    "        try:\n",
    "            update_with_query_conf(args)\n",
    "            conf = config.conf_dict\n",
    "    \n",
    "            if \"embedding_file\" in args and args[\"embedding_file\"] is not None:\n",
    "                logger.debug(\"Using embedding file for prediction.\")\n",
    "                uploaded_file = args[\"embedding_file\"]\n",
    "                print(dir(uploaded_file))\n",
    "                print(uploaded_file)\n",
    "                embedding = torch.load(uploaded_file.filename, map_location=\"cpu\")\n",
    "    \n",
    "                logger.debug(\"Loaded embedding: %s\", type(embedding))\n",
    "    \n",
    "            else:\n",
    "                return {\"error\": \"No embedding file provided\"}\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during prediction\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "        model = model_loader(device, freeze)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if isinstance(embedding, torch.Tensor):\n",
    "                embedding = embedding.to(device)\n",
    "                if freeze:\n",
    "                    output = model(embedding)[0]\n",
    "                else:\n",
    "                    output=model(embedding)\n",
    "                predicted_class = torch.argmax(output, dim=1).item()\n",
    "                probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "        print(predicted_class, probabilities)\n",
    "        return predicted_class, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2330a-27e1-4230-a532-21f4b67a44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, dataloader\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as F_general\n",
    "import scipy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ec9a66-3f1d-4ac1-9c5c-8eece98abdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from transformers import ClapProcessor, ClapAudioModelWithProjection\n",
    "\n",
    "# Configuration\n",
    "duration = 10  # in seconds\n",
    "overlap = 0.2\n",
    "desired_fs = 48000\n",
    "channel = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "wav_path=\"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-04-18_23-11-14_414-573716_Cargo_underway-using-engine_11-0_4116.wav\"\n",
    "# Load waveform\n",
    "waveform_info = torchaudio.info(wav_path)\n",
    "waveform, fs = torchaudio.load(wav_path)\n",
    "\n",
    "# Resample if needed\n",
    "if waveform_info.sample_rate != desired_fs:\n",
    "    transform = torchaudio.transforms.Resample(fs, desired_fs)\n",
    "    waveform = transform(waveform)\n",
    "\n",
    "# Crop to duration\n",
    "max_samples = int(duration * desired_fs)\n",
    "\n",
    "waveform = waveform[channel, :max_samples]\n",
    "if waveform.shape[0] < max_samples:\n",
    "    waveform = F.pad(waveform, (0, max_samples - waveform.shape[0]))\n",
    "\n",
    "# Convert to numpy for processor\n",
    "x = waveform.cpu().numpy()\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = ClapProcessor.from_pretrained(\"davidrrobinson/BioLingual\")\n",
    "clap = ClapAudioModelWithProjection.from_pretrained(\"davidrrobinson/BioLingual\")\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor(audios=[x], return_tensors=\"pt\", sampling_rate=desired_fs, padding=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Extract embedding\n",
    "with torch.no_grad():\n",
    "    embedding = clap(**inputs).audio_embeds\n",
    "\n",
    "print(\"Embedding shape:\", embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dafb4df7-006a-46d5-a244-69958c60bdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[[-13.7938, -25.0670, -30.8477,  ..., -71.3228, -72.6393, -73.0197],\n",
       "           [-17.4376, -30.2243, -28.7011,  ..., -70.8276, -70.6938, -74.5322],\n",
       "           [-14.8977, -17.4589, -25.4297,  ..., -72.3169, -72.0772, -74.7103],\n",
       "           ...,\n",
       "           [-10.7738, -22.5828, -35.8657,  ..., -69.8194, -69.4586, -70.4114],\n",
       "           [-21.1377, -18.6674, -23.5634,  ..., -72.2061, -72.6141, -72.7676],\n",
       "           [-17.5255, -32.5644, -27.3919,  ..., -72.6935, -71.4219, -72.6704]]]]),\n",
       " 'is_longer': tensor([[False]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbff96e2-ae6d-44ed-a9bc-f846eef7b1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63ba1bd-2528-4973-8831-bc6aa833dd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4498b2f-d4ba-4caf-ad2d-b2ab2c128161",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.processor = AutoProcessor.from_pretrained(\"davidrrobinson/BioLingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3312b6-8e09-4dd0-9626-533fbacdc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration: 10  # in seconds\n",
    "overlap: 0.2  # overlap of the chunks in %\n",
    "desired_fs: 48000  # desired sampling rate\n",
    "channel: 2  # number of channels\n",
    "log: True  # whether to log or not\n",
    "color: 'blue'  # color setting\n",
    " wav_path=\"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-04-18_23-11-14_414-573716_Cargo_underway-using-engine_11-0_4116.wav\"\n",
    "\n",
    "waveform_info = torchaudio.info(wav_path)\n",
    "\n",
    "# If the selection is in between two files, open both and concatenate them\n",
    "waveform, fs = torchaudio.load(wav_path) #,\n",
    "                                # num_frames=461472)\n",
    "# waveform, fs = torchaudio.load(wav_path,\n",
    "#                                 frame_offset=row['begin_sample'],\n",
    "#                                 num_frames=row['end_sample'] - row[\n",
    "#                                     'begin_sample'])\n",
    "if waveform_info.sample_rate != self.desired_fs:\n",
    "    transform = torchaudio.transforms.Resample(fs, self.desired_fs)\n",
    "    waveform = transform(waveform)\n",
    "else:\n",
    "    waveform = waveform\n",
    "\n",
    "max_samples = self.max_duration * self.desired_fs\n",
    "waveform = waveform[self.channel, :max_samples]\n",
    "if waveform.shape[0] < max_samples:\n",
    "    waveform = F_general.pad(waveform, (0, max_samples - waveform.shape[0]))\n",
    "# return wav_path torch.tensor(self.label_to_id[row['label']])\n",
    "x=waveform\n",
    "\n",
    "# Ensure input data is on the correct device\n",
    "x = [s.cpu().numpy() for s in x]\n",
    "# Process inputs with the processor\n",
    "inputs = self.processor(audios=x, return_tensors=\"pt\", sampling_rate=self.sr, padding=True)\n",
    "inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "if self.freeze_clap:\n",
    "    # Get embeddings from the CLAP model\n",
    "    out = self.clap(**inputs).audio_embeds.to(self.device)\n",
    "    # Save the computed embedding\n",
    "    embedding=out\n",
    "else:\n",
    "    embedding=inputs[\"input_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9a3d8-102e-4017-9c3d-0cfe2b1c39e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fd0e9f-7fe7-4c4b-8026-b3800e102dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Load the .pt file -----\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768 (1).pt\"\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1679e0-72fd-4610-9624-11b977fb4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "def return_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Selected CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "device=return_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9bd4e4-f23b-498c-a34e-813beba5f707",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1001x64 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     output = model(embedding)[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     output=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m predicted_class = torch.argmax(output, dim=\u001b[32m1\u001b[39m).item()\n\u001b[32m     12\u001b[39m probabilities = F.softmax(output, dim=\u001b[32m1\u001b[39m).squeeze().cpu().tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/DEEP-OC-underwater-noise-classification/audio_vessel_classifier/models.py:95\u001b[39m, in \u001b[36mFeature_extraction_CLAPModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     92\u001b[39m     x = x.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m     97\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1001x64 and 512x256)"
     ]
    }
   ],
   "source": [
    "model = model_loader(device, freeze=True)\n",
    "model.eval()\n",
    "freeze=False\n",
    "with torch.no_grad():\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding = embedding.to(device)\n",
    "        if freeze:\n",
    "            output = model(embedding)[0]\n",
    "        else:\n",
    "            output=model(embedding)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "print(predicted_class, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31560b6d-e838-4dff-8984-a90bf9aeb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, ClapModel, ClapAudioModelWithProjection, ClapProcessor\n",
    "model = model_loader(device, freeze=False)\n",
    "model.eval()\n",
    "\n",
    "# ----- Run prediction -----\n",
    "with torch.no_grad():\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding = embedding.to(device)\n",
    "        output = model(embedding)\n",
    "        print(output)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "\n",
    "        print(f\"✅ Predicted class: {predicted_class}\")\n",
    "        print(f\"✅ Class probabilities: {probabilities}\")\n",
    "    else:\n",
    "        print(\"❌ Loaded object is not a tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4e7e5-6685-4429-a129-9413fbaf51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Selected CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "device=return_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba873b-035b-4d40-b473-264458c74d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
