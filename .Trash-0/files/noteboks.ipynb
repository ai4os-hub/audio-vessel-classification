{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c51be6a-528b-44ac-bb11-4d503a392472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_vessel_classifier import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2037c532-10f1-484d-9dab-cdddd290ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_vessel_classifier import config\n",
    "CONF = config.CONF\n",
    "\n",
    "# Check and save the configuration\n",
    "config.check_conf(conf=CONF)\n",
    "config.conf_dict = config.get_conf_dict(conf=CONF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93d3940-ca9d-4fd8-946a-1b37e10452eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 1001, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "\n",
    "# Path to your .pt file\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768 (1).pt\"\n",
    "\n",
    "# Read the file content as bytes (simulating upload)\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "# Wrap in a BytesIO object to simulate file upload\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "\n",
    "# Load the tensor\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")\n",
    "\n",
    "# Show basic info\n",
    "print(type(embedding))\n",
    "print(getattr(embedding, 'shape', 'No shape (not a tensor?)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48a969a-6894-49ec-8394-9fdfb7dba5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audio_vessel_classifier import config\n",
    "from audio_vessel_classifier.misc import _catch_error\n",
    "from transformers import AutoProcessor, ClapModel, ClapAudioModelWithProjection, ClapProcessor\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cec7b8f-e1b0-47de-ac5a-d781e5e15e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=11, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_model = ClapAudioModelWithProjection.from_pretrained(\n",
    "    \"/srv/DEEP-OC-underwater-noise-classification/models/fine_tuning/model\"\n",
    ").to(device)\n",
    "clap_model.eval()\n",
    "\n",
    "# Load the linear classification head\n",
    "linear_model = nn.Linear(in_features=512, out_features=11)\n",
    "linear_model.load_state_dict(torch.load(\n",
    "    \"/srv/DEEP-OC-underwater-noise-classification/models/fine_tuning/model/linear.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "linear_model = linear_model.to(device)\n",
    "linear_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a623564-5eeb-4b15-b540-4c8f832cf78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "def return_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Selected CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "device=return_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9631eb66-db19-4e04-a393-807353308ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 1001, 64])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clap_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     raw_input = raw_input.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     clap_output = \u001b[43mclap_model\u001b[49m(raw_input.to(device))\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Pass through linear classification head\u001b[39;00m\n\u001b[32m     29\u001b[39m     output = linear_model(clap_output.audio_embeds)\n",
      "\u001b[31mNameError\u001b[39m: name 'clap_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "\n",
    "# Path to your .pt file\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768 (1).pt\"\n",
    "\n",
    "# Read the file content as bytes (simulating upload)\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "# Wrap in a BytesIO object to simulate file upload\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "\n",
    "# Load the tensor\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")\n",
    "\n",
    "# Show basic info\n",
    "print(type(embedding))\n",
    "print(getattr(embedding, 'shape', 'No shape (not a tensor?)'))\n",
    "\n",
    "raw_input=embedding\n",
    "if raw_input.dim() == 2:  # assuming (features, time)\n",
    "    raw_input = raw_input.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    clap_output = clap_model(raw_input.to(device))\n",
    "\n",
    "    # Pass through linear classification head\n",
    "    output = linear_model(clap_output.audio_embeds)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    probabilities = torch.softmax(output, dim=1).squeeze().cpu().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376de214-0585-49be-961a-7f4ad7887b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Fine_tuning_CLAPModel(nn.Module):\n",
    "    def __init__(self, clap_model, linear_model):\n",
    "        super().__init__()\n",
    "        self.clap_model = clap_model\n",
    "        self.linear_model = linear_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:  # (features, time)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.clap_model(x.to(x.device))\n",
    "\n",
    "        embeddings = output.audio_embeds\n",
    "        logits = self.linear_model(embeddings)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Feature_extraction_CLAPModel(nn.Module):\n",
    "    def __init__(self, fc1,fc2,relu):\n",
    "        super().__init__()\n",
    "        self.fc1 = fc1\n",
    "        self.fc2 = fc2\n",
    "        self.relu=relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:  # (features, time)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def model_loader(device, freeze=True):\n",
    "    if freeze:\n",
    "        fc1 = nn.Linear(in_features=512, out_features=256)\n",
    "        fc2 = nn.Linear(in_features=256, out_features=11)\n",
    "        relu = nn.ReLU()\n",
    "    \n",
    "    \n",
    "        fc1.load_state_dict(torch.load(\n",
    "            \"/srv/DEEP-OC-underwater-noise-classification/models/feature_extraction/model/fc1.pth\",\n",
    "            map_location=device\n",
    "        ))\n",
    "    \n",
    "        fc2.load_state_dict(torch.load(\n",
    "            \"/srv/DEEP-OC-underwater-noise-classification/models/feature_extraction/model/fc2.pth\",\n",
    "            map_location=device\n",
    "        ))\n",
    "    \n",
    "        relu = nn.ReLU()\n",
    "        model = Feature_extraction_CLAPModel(fc1, fc2, relu).to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    else:    \n",
    "        # Load CLAP model\n",
    "        clap_model = ClapAudioModelWithProjection.from_pretrained(\n",
    "            \"/srv/DEEP-OC-underwater-noise-classification/models/fine_tuning/model\"\n",
    "        ).to(device)\n",
    "        clap_model.eval()\n",
    "    \n",
    "        # Load linear head\n",
    "        linear_model = nn.Linear(in_features=512, out_features=11)\n",
    "        linear_model.load_state_dict(torch.load(\n",
    "            \"/srv/DEEP-OC-underwater-noise-classification/models/fine_tuning/model/linear.pth\",\n",
    "            map_location=device\n",
    "        ))\n",
    "        linear_model = linear_model.to(device)\n",
    "        linear_model.eval()\n",
    "        model=Fine_tuning_CLAPModel(clap_model, linear_model).to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "\n",
    "class Feature_extraction_CLAPModel(nn.Module):\n",
    "    def __init__(self, fc1,fc2,relu):\n",
    "        super().__init__()\n",
    "        self.fc1 = fc1\n",
    "        self.fc2 = fc2\n",
    "        self.relu=relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:  # (features, time)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9450d8b-ffa1-47e8-9c89-d1d4ba036b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded object type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 512])\n",
      "1\n",
      "tensor([ 0.1399,  0.1999,  0.2344,  0.2434,  0.2392,  0.2089,  0.1574,  0.0785,\n",
      "        -0.0233, -0.1908, -0.4025])\n",
      "✅ Predicted class: 3\n",
      "✅ Class probabilities: [0.09475743025541306, 0.10062021017074585, 0.104147769510746, 0.10509079694747925, 0.1046474352478981, 0.10153113305568695, 0.09643180668354034, 0.08911938220262527, 0.08048717677593231, 0.06807927042245865, 0.05508754029870033]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Load the .pt file -----\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768.pt\"\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")\n",
    "\n",
    "print(f\"Loaded object type: {type(embedding)}\")\n",
    "print(f\"Shape: {getattr(embedding, 'shape', 'No shape')}\")\n",
    "\n",
    "# ----- Prepare the model -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_loader(device, freeze=True)\n",
    "model.eval()\n",
    "\n",
    "# ----- Run prediction -----\n",
    "with torch.no_grad():\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        print(len(embedding))\n",
    "        embedding = embedding.to(device)\n",
    "        output = model(embedding)[0]\n",
    "        print(output[0])\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "\n",
    "        print(f\"✅ Predicted class: {predicted_class}\")\n",
    "        print(f\"✅ Class probabilities: {probabilities}\")\n",
    "    else:\n",
    "        print(\"❌ Loaded object is not a tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871dc8b5-e7b0-4b39-9751-ea8dd8e4713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded object type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 1001, 64])\n",
      "tensor([[[[-16.4730, -32.7780, -26.3730,  ..., -62.5544, -61.8940, -62.5178],\n",
      "          [-15.5435, -30.7138, -33.1804,  ..., -62.8598, -61.7451, -65.3546],\n",
      "          [ -6.8622, -17.5188, -31.9298,  ..., -61.4319, -61.5811, -62.9530],\n",
      "          ...,\n",
      "          [ -9.4487, -25.2350, -25.3325,  ..., -60.9797, -64.2304, -62.7755],\n",
      "          [-14.5285, -22.4340, -22.2966,  ..., -60.7422, -61.3189, -63.8114],\n",
      "          [-11.2158,  -8.8075, -13.1319,  ..., -62.9723, -60.0463, -65.6816]]]])\n",
      "tensor([[ 0.1266,  0.4500,  0.6062,  0.6953,  0.6020,  0.3639,  0.0129, -0.4810,\n",
      "         -0.9104, -1.1478, -1.4659]])\n",
      "✅ Predicted class: 3\n",
      "✅ Class probabilities: [0.09162174165248871, 0.12660335004329681, 0.1480112224817276, 0.16180580854415894, 0.14738497138023376, 0.116162970662117, 0.08177360892295837, 0.049903158098459244, 0.03248002752661705, 0.025616005063056946, 0.018637174740433693]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Load the .pt file -----\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768 (1).pt\"\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")\n",
    "\n",
    "print(f\"Loaded object type: {type(embedding)}\")\n",
    "print(f\"Shape: {getattr(embedding, 'shape', 'No shape')}\")\n",
    "\n",
    "# ----- Prepare the model -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_loader(device, freeze=False)\n",
    "model.eval()\n",
    "\n",
    "# ----- Run prediction -----\n",
    "with torch.no_grad():\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding = embedding.to(device)\n",
    "        print(embedding)\n",
    "        output = model(embedding)\n",
    "        print(output)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "\n",
    "        print(f\"✅ Predicted class: {predicted_class}\")\n",
    "        print(f\"✅ Class probabilities: {probabilities}\")\n",
    "    else:\n",
    "        print(\"❌ Loaded object is not a tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6624f179-e6a8-49b6-922d-5bdfd92aa861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1399,  0.1999,  0.2344,  0.2434,  0.2392,  0.2089,  0.1574,\n",
       "           0.0785, -0.0233, -0.1908, -0.4025]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70f70e5d-f42a-4de0-937c-8f5fc9cf375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded object type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 1001, 64])\n",
      "✅ Predicted class: 3\n",
      "✅ Class probabilities: [0.09162174165248871, 0.12660335004329681, 0.1480112224817276, 0.16180580854415894, 0.14738497138023376, 0.116162970662117, 0.08177360892295837, 0.049903158098459244, 0.03248002752661705, 0.025616005063056946, 0.018637174740433693]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Load the .pt file -----\n",
    "pt_path = \"/srv/DEEP-OC-underwater-noise-classification/data/G_15810_2022-02-07_08-21-29_156-760693_Cargo_underway-using-engine_12-3_1768 (1).pt\"\n",
    "with open(pt_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "file_like = io.BytesIO(file_bytes)\n",
    "embedding = torch.load(file_like, map_location=\"cpu\")\n",
    "\n",
    "print(f\"Loaded object type: {type(embedding)}\")\n",
    "print(f\"Shape: {getattr(embedding, 'shape', 'No shape')}\")\n",
    "\n",
    "# ----- Prepare the model -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_loader(device)\n",
    "model.eval()\n",
    "\n",
    "# ----- Run prediction -----\n",
    "with torch.no_grad():\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding = embedding.to(device)\n",
    "        output = model(embedding)\n",
    "\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        probabilities = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "\n",
    "        print(f\"✅ Predicted class: {predicted_class}\")\n",
    "        print(f\"✅ Class probabilities: {probabilities}\")\n",
    "    else:\n",
    "        print(\"❌ Loaded object is not a tensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1db04323-14ff-47c2-a9ea-6150a994293d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mmodel_loader\u001b[49m()\n\u001b[32m      2\u001b[39m logits = model(embedding.to(device))\n\u001b[32m      3\u001b[39m predicted_class = torch.argmax(logits, dim=\u001b[32m1\u001b[39m).item()\n",
      "\u001b[31mNameError\u001b[39m: name 'model_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_loader()\n",
    "logits = model(embedding.to(device))\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "probabilities = torch.softmax(logits, dim=1).squeeze().cpu().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a32792bd-1080-4d18-a971-4407ae99e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b1e274-016a-4d46-8dce-09ab1c5530f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09162174165248871,\n",
       " 0.12660335004329681,\n",
       " 0.1480112224817276,\n",
       " 0.16180580854415894,\n",
       " 0.14738497138023376,\n",
       " 0.116162970662117,\n",
       " 0.08177360892295837,\n",
       " 0.049903158098459244,\n",
       " 0.03248002752661705,\n",
       " 0.025616005063056946,\n",
       " 0.018637174740433693]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f273728-bec7-46b5-a916-1b7b70fa0e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clap_output.audio_embeds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4506ce74-b5ff-4976-8d1a-ec9e978e4f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8619e-02, -1.3301e-01,  2.0999e-01, -2.2199e-01,  2.1193e-02,\n",
       "         -3.1606e-01,  2.1937e-01, -2.1222e-01, -2.4672e-01,  4.3776e-03,\n",
       "         -3.0062e-01,  1.5420e-01,  1.5935e-01,  5.8744e-02, -6.6256e-03,\n",
       "         -3.9827e-01, -2.8521e-01, -1.5809e-01, -3.1395e-01,  4.2836e-01,\n",
       "          1.2994e-01, -1.1704e-01, -4.5699e-01,  4.1617e-02,  6.7006e-02,\n",
       "         -3.5563e-02, -5.7896e-02, -4.2090e-03, -1.6218e-01, -3.9658e-02,\n",
       "          4.0642e-01,  1.6578e-01, -3.1217e-01,  7.3475e-03, -2.7456e-01,\n",
       "         -1.2519e-01,  2.4036e-01, -3.5438e-01, -1.8330e-01,  1.8214e-01,\n",
       "         -4.9875e-02, -1.0472e-01, -1.1271e-02,  1.8519e-02,  3.6128e-01,\n",
       "         -8.7712e-02,  1.5493e-01, -1.2605e-01, -3.3798e-02,  2.4694e-01,\n",
       "         -7.2173e-02, -3.5188e-02, -2.7408e-01,  2.0994e-02, -4.3836e-02,\n",
       "          4.5556e-02, -6.5583e-02,  4.4070e-02, -1.3603e-02, -2.4973e-01,\n",
       "         -2.9477e-01, -2.4779e-01, -1.1213e-01, -1.0888e-01,  7.8344e-02,\n",
       "         -1.8834e-02,  2.6447e-01, -1.7645e-01, -4.2171e-02, -4.1326e-02,\n",
       "         -6.7060e-01, -1.4850e-01, -2.6645e-01,  1.3072e-01,  5.8726e-02,\n",
       "         -2.7061e-01,  1.4006e-01,  3.8375e-02,  1.4921e-01,  4.4812e-02,\n",
       "          8.9391e-02, -6.5700e-02,  1.6902e-01, -3.6500e-01, -1.3036e-01,\n",
       "         -4.2177e-01, -1.5643e-01, -2.6965e-01, -1.7039e-01,  1.2697e-01,\n",
       "          1.5556e-02, -1.9580e-01,  1.7532e-01, -4.2392e-02,  8.0676e-02,\n",
       "         -1.5163e-01, -2.8827e-01, -1.9072e-01,  2.7194e-01, -5.7275e-02,\n",
       "          9.7512e-02, -1.3811e-01,  3.1860e-01,  3.4532e-03, -1.3233e-02,\n",
       "          1.9735e-01,  2.5345e-01, -9.2111e-02, -5.2580e-01, -1.1124e-01,\n",
       "          6.7787e-02,  2.7112e-01,  2.3223e-01, -1.5540e-01,  1.4285e-01,\n",
       "          1.3753e-01,  1.1587e-02,  2.8977e-01,  8.0118e-03, -1.2362e-01,\n",
       "          1.5543e-01,  1.7318e-01,  1.7837e-02, -5.4581e-02,  4.5009e-01,\n",
       "          3.1224e-01, -1.3396e-01,  1.5698e-02,  3.0067e-01, -4.8270e-01,\n",
       "          2.8682e-01, -2.1463e-01, -2.3932e-01,  3.2828e-01, -1.0980e-01,\n",
       "          1.3149e-01,  1.8656e-01,  6.8602e-02, -1.3541e-01, -4.2898e-01,\n",
       "         -1.8835e-01,  6.3372e-03,  3.3722e-01, -4.8986e-01, -3.8473e-01,\n",
       "          1.0617e-01, -5.1737e-02,  5.4186e-02,  1.2863e-01,  6.6566e-02,\n",
       "         -2.4665e-01,  2.4679e-01,  7.1775e-03, -4.2712e-01, -8.4671e-02,\n",
       "         -7.2090e-02,  1.5573e-01, -3.5989e-01, -2.1185e-02, -7.3086e-02,\n",
       "         -2.8473e-01,  1.5180e-03, -2.0085e-01, -6.9923e-02, -4.0226e-02,\n",
       "          9.3524e-03, -1.1785e-01,  5.4249e-02,  6.4046e-01,  2.0630e-02,\n",
       "          1.3415e-02,  1.9594e-01, -1.8664e-01, -3.0449e-01, -2.3010e-01,\n",
       "          1.4849e-03,  1.4180e-02, -4.1235e-01, -1.4464e-01,  2.6230e-02,\n",
       "         -1.8350e-02,  6.4259e-02, -5.4793e-02,  2.9473e-01, -1.1783e-02,\n",
       "          2.5269e-01, -1.1608e-01,  4.0566e-02, -2.7633e-01, -4.2905e-01,\n",
       "         -3.2413e-03, -1.0177e-02, -5.7593e-02,  2.6062e-02,  7.4226e-03,\n",
       "         -3.7026e-02,  2.1337e-01,  2.6087e-02, -1.1074e-01, -9.4511e-02,\n",
       "         -3.0863e-01, -5.2980e-02,  1.7583e-01,  4.0085e-01,  3.0419e-02,\n",
       "         -3.7160e-03, -1.0467e-01,  9.6069e-02,  6.1368e-02, -3.3064e-01,\n",
       "          3.6270e-02, -1.9396e-01, -3.9575e-03, -2.2715e-01,  9.8028e-03,\n",
       "         -1.4719e-02, -1.8777e-01,  5.6197e-02, -1.5670e-01, -3.5418e-01,\n",
       "          2.2275e-02, -4.0534e-02,  1.2392e-01,  1.5201e-01,  4.2903e-01,\n",
       "         -1.8559e-03, -1.4044e-02, -5.8838e-02, -3.6664e-01, -1.7876e-02,\n",
       "          4.5189e-02, -7.0594e-02,  1.9045e-01, -3.3811e-01,  1.3530e-01,\n",
       "          1.1313e-01,  3.3923e-03,  4.5997e-01, -4.6988e-01, -2.9464e-02,\n",
       "          1.7160e-01, -8.2857e-02, -8.5158e-02,  1.2315e-01, -2.6427e-01,\n",
       "         -9.5047e-02, -3.2652e-03, -4.0853e-02, -2.4365e-01, -1.6967e-01,\n",
       "          1.7681e-01,  4.3303e-01,  4.5597e-01,  8.5995e-02,  2.9946e-01,\n",
       "         -4.0480e-01, -1.9999e-01,  1.9038e-01, -4.3173e-02,  9.1214e-02,\n",
       "          5.4641e-02,  1.0840e-02, -8.1799e-02, -2.0830e-02, -3.1294e-01,\n",
       "         -5.8272e-02,  1.5670e-01, -3.3556e-02,  6.8164e-02,  3.3791e-01,\n",
       "         -4.8583e-01, -1.3460e-01, -1.5613e-02,  3.2234e-01,  9.1287e-02,\n",
       "         -1.2998e-01, -4.1941e-02, -5.2062e-01, -2.1462e-01,  1.0797e-01,\n",
       "         -2.5354e-01,  1.6434e-02,  1.0636e-01, -1.7043e-02, -2.4940e-01,\n",
       "          7.8013e-02,  3.6211e-01, -8.9750e-02, -1.6056e-01, -2.9517e-02,\n",
       "          2.0779e-01,  1.9678e-01, -3.3472e-02, -2.0480e-01,  9.9019e-02,\n",
       "         -1.7279e-01, -8.9495e-02,  1.6961e-01,  2.3649e-01, -3.4164e-01,\n",
       "         -6.9210e-02,  2.9253e-01, -6.9014e-02, -3.4819e-01,  4.6823e-01,\n",
       "         -5.7536e-02,  4.1541e-02, -4.7790e-01,  2.0807e-02, -8.0542e-02,\n",
       "          6.3138e-02,  2.1655e-01, -8.2780e-02,  4.3082e-01, -3.4712e-01,\n",
       "          2.6739e-01, -3.5384e-02,  4.0424e-01, -2.3250e-01,  2.8473e-01,\n",
       "          4.5835e-01, -1.5337e-01,  2.4396e-01,  9.2538e-02,  1.0712e-01,\n",
       "         -8.5903e-03, -5.9644e-03, -5.6415e-02,  2.6938e-01, -8.1876e-02,\n",
       "          1.8933e-01, -2.5997e-01, -2.8457e-01,  4.9425e-03, -4.6173e-01,\n",
       "         -1.2400e-02, -1.4006e-01,  1.8409e-01, -5.5162e-02,  6.6555e-01,\n",
       "          1.9228e-01,  9.5391e-02,  5.1283e-02,  1.7080e-01, -5.3655e-01,\n",
       "          1.5265e-01, -2.3494e-01, -3.9098e-01, -9.2861e-02, -2.6537e-01,\n",
       "         -3.1534e-02, -2.0878e-01,  9.0350e-02,  3.9054e-02,  1.5156e-02,\n",
       "         -3.1534e-01, -3.1793e-02,  8.0284e-03, -5.3614e-02,  1.8053e-02,\n",
       "         -1.1435e-01,  2.7841e-01, -1.6001e-01,  2.2887e-01,  5.1671e-01,\n",
       "          5.4004e-01,  3.3892e-02,  6.5948e-01, -3.7899e-02, -9.6789e-02,\n",
       "          9.7743e-02,  7.1322e-02,  1.9105e-01,  1.4809e-01, -7.6510e-03,\n",
       "         -5.3158e-01, -1.0070e-02, -1.2614e-01, -2.5712e-02, -1.9735e-01,\n",
       "          5.3116e-01, -4.7233e-02, -1.6573e-01,  1.6757e-01, -4.2942e-02,\n",
       "         -2.5877e-01,  2.4057e-02,  1.7932e-01,  1.0381e-02,  1.0897e-01,\n",
       "         -3.3652e-01, -3.2604e-01,  1.6846e-01,  3.4294e-01,  6.4604e-01,\n",
       "          9.4179e-02, -1.4041e-01,  5.3153e-01, -3.0086e-02, -3.7639e-01,\n",
       "         -5.5190e-02, -1.5676e-01, -4.0621e-01, -2.1610e-01, -1.6989e-01,\n",
       "         -1.6121e-01,  3.4725e-02,  1.3395e-01,  6.3259e-02, -5.7762e-01,\n",
       "          1.1970e-02,  5.7576e-02,  2.7584e-01, -3.3006e-01, -3.5186e-01,\n",
       "          4.1667e-01, -1.8156e-01, -3.3808e-01, -1.2259e-03, -1.7463e-01,\n",
       "          4.7401e-01,  3.2420e-02,  2.6729e-01, -1.4938e-02,  8.1346e-02,\n",
       "          5.7643e-02,  6.8287e-02, -3.3955e-01,  7.5495e-03,  1.6718e-01,\n",
       "         -1.7551e-02,  1.9632e-01, -3.0094e-01, -5.3091e-02,  2.3059e-01,\n",
       "          3.5542e-01, -8.8223e-02, -5.7244e-02,  2.0466e-02,  1.8720e-01,\n",
       "          5.9365e-02,  6.8174e-04,  3.8592e-01, -1.0111e-01,  8.7030e-02,\n",
       "          1.4462e-01,  2.6005e-01, -2.5557e-01,  6.9435e-02,  8.8597e-02,\n",
       "          2.7005e-02,  2.0742e-01,  2.6852e-01, -1.5260e-01,  9.8638e-02,\n",
       "         -1.0408e-01, -2.8484e-02,  8.6274e-02,  1.5584e-02, -1.2824e-01,\n",
       "          6.5333e-02, -7.7152e-02, -1.7536e-01, -3.3671e-01,  4.2640e-02,\n",
       "         -1.9122e-01,  4.1300e-03,  1.1098e-01,  1.1956e-01,  3.2574e-01,\n",
       "         -5.6755e-02,  1.2187e-01,  3.6998e-02,  1.6144e-01,  1.2369e-01,\n",
       "          7.4795e-03, -3.2983e-01, -1.6496e-02,  2.7613e-01, -5.3009e-02,\n",
       "          1.2667e-01,  3.5863e-01, -1.6618e-02,  3.9220e-02, -1.5528e-01,\n",
       "         -5.4408e-01, -2.0151e-01, -5.4095e-01, -8.2904e-02,  2.6130e-02,\n",
       "          3.8518e-02, -1.1478e-01, -4.4175e-02, -1.2262e-02, -8.8348e-02,\n",
       "          9.8095e-02,  1.4111e-01, -5.2541e-02, -3.1194e-02, -6.7635e-02,\n",
       "         -8.1347e-02, -4.3746e-02, -5.1125e-02,  1.1946e-01,  5.0787e-02,\n",
       "         -3.4504e-01, -3.3443e-01, -7.3937e-01, -8.9199e-02,  1.1056e-01,\n",
       "         -1.1204e-01,  1.5205e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_output.audio_embeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
